{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=None, sigma=False, bias=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "        if output_dim != None:\n",
    "            self.linear_2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "        self.sigma = sigma\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        if self.sigma and self.output_dim != None:\n",
    "            x = F.relu(x)\n",
    "            x = self.linear_2(x)\n",
    "        return (x)\n",
    "    \n",
    "\n",
    "    \n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, x, y, mlp, learning_rate, node_id):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y.view(1)\n",
    "        self.mlp = mlp\n",
    "        self.node_id = node_id\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.SGD(self.mlp.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "    def receive_params(self, parameters):\n",
    "        \n",
    "        # parameters: a dict of parameters: {\"linear_1.weight\": tensor,......}\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.mlp.named_parameters():\n",
    "                param.copy_(parameters[name])\n",
    "                \n",
    "        \n",
    "    def upload_params(self):\n",
    "        \n",
    "        # return a dict of parameters: {\"linear_1,weight\":\" tensor,......,}\n",
    "        return mlp.state_dict()\n",
    "    \n",
    "    # Use this funtion only at the beginning of each communication\n",
    "    def upload_h(self): \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        h = self.mlp(self.x)\n",
    "        return h\n",
    "    \n",
    "    def receive_and_update(self, Abar_v, h_u, h_v, E):\n",
    "        \n",
    "        \n",
    "        # h_u is the aggregated information of the neighboors of node v, it is constant\n",
    "        # Abar_v is the corresponding constant for h_v and it is constant\n",
    "        # E is the number of local updates\n",
    "        \n",
    "        # Note that: this function can only deal with node has only one set of data\n",
    "        \n",
    "        Z_v = Abar_v*h_v + h_u\n",
    "        yhat_v = F.log_softmax(Z_v, dim=0)\n",
    "        loss = F.nll_loss(yhat_v.view(1,-1), self.y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        for e in range(E-1):\n",
    "            self.optimizer.zero_grad()\n",
    "            h_v = self.mlp(self.x)\n",
    "            Z_v = (Abar_v*h_v + h_u)\n",
    "            yhat_v = F.log_softmax(Z_v, dim=0)\n",
    "            loss = F.nll_loss(yhat_v.view(1,-1), self.y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class Central_Server:\n",
    "    \n",
    "    def __init__(self, edge_index, node_list, A, K, alpha=0.95):\n",
    "        \n",
    "        self.edge_index = edge_index\n",
    "        self.node_list = node_list\n",
    "        self.A = A\n",
    "        self.alpha = alpha\n",
    "        self.K = K\n",
    "        self.Abar = None\n",
    "        self.central_params = None\n",
    "        self.num_nodes = len(node_list)\n",
    "        \n",
    "    def testing_mlp(self, input_dim, hidden_dim, output_dim=None, sigma=False):\n",
    "        self.mlp = MLP(input_dim, hidden_dim, output_dim, sigma)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def receive_h(self):\n",
    "        \n",
    "        H = []\n",
    "        for v in self.node_list:\n",
    "            h_v = v.upload_h()\n",
    "            H.append(h_v)\n",
    "        return H\n",
    "    \n",
    "    def compute_Abar(self):\n",
    "        \n",
    "        N = self.num_nodes\n",
    "        A_bar = torch.zeros((N,N))\n",
    "        A_i = torch.diag(torch.ones(N))\n",
    "        alpha_i = 1\n",
    "        for i in range(0, self.K+1):\n",
    "            A_bar = A_bar + alpha_i*A_i\n",
    "            alpha_i = alpha_i * self.alpha\n",
    "            A_i = torch.matmul(A_i, self.A)\n",
    "        A_bar = (1-self.alpha)*A_bar\n",
    "        self.Abar = A_bar\n",
    "    \n",
    "    def init_params(self):\n",
    "        \n",
    "        \n",
    "        params = copy.deepcopy(self.node_list[0].mlp.state_dict())\n",
    "        self.cparam_names = list(params.keys())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param_name in params.keys():\n",
    "                nn.init.normal_(params[param_name])\n",
    "        self.central_params = params\n",
    "        \n",
    "        \n",
    "            \n",
    "    def one_communication(self, E):\n",
    "        \n",
    "        # Calculate the A_bar matrix, if it is not calculated already.\n",
    "        if (self.Abar == None):\n",
    "            self.compute_Abar()\n",
    "            \n",
    "        # If this is the first time, initilalize the central parameters.\n",
    "        if (self.central_params == None):\n",
    "            self.init_params()\n",
    "            \n",
    "        # Broadcast the current central parameters to all nodes:\n",
    "        for v in self.node_list:\n",
    "            v.receive_params(self.central_params)\n",
    "            \n",
    "        # forward through the MLP layers for each node\n",
    "        H = self.receive_h()\n",
    "        \n",
    "        # Power iteration\n",
    "        H_copy = torch.stack(H).clone().detach()\n",
    "        Z_K = torch.matmul(self.Abar, H_copy)\n",
    "        \n",
    "        # Shape of H\n",
    "        num_nodes, num_classes = H_copy.shape\n",
    "        \n",
    "        # Calculate the needed information for each node to get the yhat\n",
    "        for i in range(num_nodes):\n",
    "            with torch.no_grad():\n",
    "                h_u = Z_K[i,:] - H_copy[i,:]*self.Abar[i,i]\n",
    "            # Local update\n",
    "            self.node_list[i].receive_and_update(self.Abar[i,i], h_u, H[i], E)\n",
    "            \n",
    "        # Collect the updated local parameters and aggregate\n",
    "        with torch.no_grad():\n",
    "            for pname in self.cparam_names:\n",
    "                p = self.node_list[0].mlp.state_dict()[pname]\n",
    "                for i in range(1, self.num_nodes):\n",
    "                    p = p + self.node_list[i].mlp.state_dict()[pname]\n",
    "                p = p/self.num_nodes\n",
    "                self.central_params[pname] = p\n",
    "            \n",
    "            \n",
    "    def training(self, T, E, data=None):\n",
    "        \n",
    "        \n",
    "        accuracy = []\n",
    "        for t in range(T):\n",
    "            self.one_communication(E)\n",
    "            print (\"Communication:\", t+1)\n",
    "            if (data != None):\n",
    "                accuracy.append(self.training_accuracy(data, self.central_params))\n",
    "                \n",
    "        if (data != None):\n",
    "            plt.plot(np.arange(T)+1, accuracy)\n",
    "            return accuracy\n",
    "                \n",
    "            \n",
    "    def training_accuracy(self, graph_data, params):\n",
    "        X = graph_data.x\n",
    "        Y = graph_data.y\n",
    "        self.mlp.load_state_dict(params)\n",
    "        with torch.no_grad():\n",
    "                H = self.mlp(X)\n",
    "                Z_K = torch.matmul(self.Abar, H)\n",
    "                preds = torch.max(Z_K, dim=1)[1]\n",
    "                counts = (preds == Y).sum()\n",
    "        return counts.item()/self.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(data, A, num_classes, hidden_dim, learning_rate, K,\n",
    "                       output_dim=None, sigma=False):\n",
    "    \n",
    "    num_nodes, input_dim = data.x.shape\n",
    "    node_list = []\n",
    "    \n",
    "    for v in range(num_nodes):\n",
    "        \n",
    "        if (output_dim != None and sigma == True):\n",
    "            mlp = MLP(input_dim, hidden_dim, output_dim, sigma)\n",
    "            \n",
    "        else:\n",
    "            mlp = MLP(input_dim, hidden_dim)\n",
    "            \n",
    "        node_v = Node(data.x[v,:], data.y[v], mlp, learning_rate, v)\n",
    "        node_list.append(node_v)\n",
    "    \n",
    "    network = Central_Server(data.edge_index, node_list, A, K)\n",
    "    \n",
    "    network.testing_mlp(input_dim, hidden_dim, output_dim, sigma)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 1\n",
      "Communication: 2\n",
      "Communication: 3\n",
      "Communication: 4\n",
      "Communication: 5\n",
      "Communication: 6\n",
      "Communication: 7\n",
      "Communication: 8\n",
      "Communication: 9\n",
      "Communication: 10\n",
      "Communication: 11\n",
      "Communication: 12\n",
      "Communication: 13\n",
      "Communication: 14\n",
      "Communication: 15\n",
      "Communication: 16\n",
      "Communication: 17\n",
      "Communication: 18\n",
      "Communication: 19\n",
      "Communication: 20\n",
      "Communication: 21\n",
      "Communication: 22\n",
      "Communication: 23\n",
      "Communication: 24\n",
      "Communication: 25\n",
      "Communication: 26\n",
      "Communication: 27\n",
      "Communication: 28\n",
      "Communication: 29\n",
      "Communication: 30\n",
      "Communication: 31\n",
      "Communication: 32\n",
      "Communication: 33\n",
      "Communication: 34\n",
      "Communication: 35\n",
      "Communication: 36\n",
      "Communication: 37\n",
      "Communication: 38\n",
      "Communication: 39\n",
      "Communication: 40\n",
      "Communication: 41\n",
      "Communication: 42\n",
      "Communication: 43\n",
      "Communication: 44\n",
      "Communication: 45\n",
      "Communication: 46\n",
      "Communication: 47\n",
      "Communication: 48\n",
      "Communication: 49\n",
      "Communication: 50\n",
      "Communication: 51\n",
      "Communication: 52\n",
      "Communication: 53\n",
      "Communication: 54\n",
      "Communication: 55\n",
      "Communication: 56\n",
      "Communication: 57\n",
      "Communication: 58\n",
      "Communication: 59\n",
      "Communication: 60\n",
      "Communication: 61\n",
      "Communication: 62\n",
      "Communication: 63\n",
      "Communication: 64\n",
      "Communication: 65\n",
      "Communication: 66\n",
      "Communication: 67\n",
      "Communication: 68\n",
      "Communication: 69\n",
      "Communication: 70\n",
      "Communication: 71\n",
      "Communication: 72\n",
      "Communication: 73\n",
      "Communication: 74\n",
      "Communication: 75\n",
      "Communication: 76\n",
      "Communication: 77\n",
      "Communication: 78\n",
      "Communication: 79\n",
      "Communication: 80\n",
      "Communication: 81\n",
      "Communication: 82\n",
      "Communication: 83\n",
      "Communication: 84\n",
      "Communication: 85\n",
      "Communication: 86\n",
      "Communication: 87\n",
      "Communication: 88\n",
      "Communication: 89\n",
      "Communication: 90\n",
      "Communication: 91\n",
      "Communication: 92\n",
      "Communication: 93\n",
      "Communication: 94\n",
      "Communication: 95\n",
      "Communication: 96\n",
      "Communication: 97\n",
      "Communication: 98\n",
      "Communication: 99\n",
      "Communication: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "data.edge_index = utils.remove_self_loops(data.edge_index)[0]\n",
    "data.edge_index = utils.add_remaining_self_loops(data.edge_index)[0]\n",
    "G = utils.to_networkx(data, to_undirected=True)\n",
    "A = torch.tensor(nx.linalg.graphmatrix.adjacency_matrix(G).todense()).type(torch.FloatTensor)\n",
    "D = nx.linalg.graphmatrix.adjacency_matrix(G).todense() + nx.linalg.laplacianmatrix.laplacian_matrix(G).todense()\n",
    "Dhalf = scipy.linalg.sqrtm(D)\n",
    "Dnhalf = torch.tensor(scipy.linalg.inv(Dhalf)).type(torch.FloatTensor)\n",
    "A = torch.matmul(torch.matmul(Dnhalf, A), Dnhalf)\n",
    "server = init_network(data, A, 7, 100, 1, 10, 7, True)\n",
    "accuracy = server.training(100, 1, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K: 10, 20, 50, 100\n",
    "\n",
    "Hidden: 50, 100, 200, 500\n",
    "\n",
    "Learning_rate: 0.2, 0.4, 0.6, 0.8, 1, lr(E)\n",
    "\n",
    "E: 5, 2, 2, 2, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
